{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80c32960a24f4d50b746ff3156ff5530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Click to Select a Dataset",
              "Electric Vehicle Population Data",
              "Lottery Powerball Winning Numbers: Beginning 2010",
              "Crime Data from 2020 to Present",
              "FDIC Failed Bank List",
              "U.S. Chronic Disease Indicators (CDI)",
              "Death rates for suicide, by sex, race, Hispanic origin, and age: United States",
              "Walkability Index",
              "Motor Vehicle Collisions - Crashes",
              "Supply Chain Greenhouse Gas Emission Factors v1.2 by NAICS-6",
              "Alzheimer's Disease and Healthy Aging Data",
              "Real Estate Sales 2001-2020 GL",
              "Air Quality",
              "Mental Health Care in the Last 4 Weeks",
              "Lottery Mega Millions Winning Numbers: Beginning 2002",
              "National Obesity By State",
              "Crimes - 2001 to Present",
              "Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System",
              "Drug overdose death rates, by drug type, sex, age, race, and Hispanic origin: United States",
              "Popular Baby Names",
              "Warehouse and Retail Sales",
              "U.S. Electric Utility Companies and Rates: Look-up by Zipcode (2020)",
              "Conditions Contributing to COVID-19 Deaths, by State and Age, Provisional 2020-2023",
              "Diabetes",
              "Crash Reporting - Drivers Data",
              "Consumer Complaint Database",
              "Public School Characteristics - Current",
              "Health conditions among children under age 18, by selected characteristics: United States",
              "Indicators of Anxiety or Depression Based on Reported Frequency of Symptoms During Last 7 Days",
              "Air Traffic Passenger Statistics",
              "Heart Disease Mortality Data Among US Adults (35+) by State/Territory and County",
              "Pittsburgh American Community Survey 2015 - Miscellaneous Data",
              "NYPD Shooting Incident Data (Historic)",
              "Demographic Statistics By Zip Code",
              "Obesity among children and adolescents aged 2–19 years, by selected characteristics: United States",
              "School Attendance by Student Group and District, 2021-2022",
              "Border Crossing Entry Data",
              "Meteorite Landings",
              "Electric Vehicle Population Size History By County",
              "NCHS - Leading Causes of Death: United States",
              "Traffic Crashes - Crashes",
              "Cancer Rates",
              "Air Quality Measures on the National Environmental Health Tracking Network",
              "Smart Location Database",
              "Jobs NYC Postings",
              "Adoptable Pets",
              "Allegheny County Employee Salaries",
              "Supply Chain Shipment Pricing Data",
              "Electric Vehicle Charging Stations",
              "Listing of Active Businesses",
              "Traffic Violations",
              "NCHS - Death rates and life expectancy at birth",
              "Percent Change in Consumer Spending",
              "Specific Chronic Conditions",
              "Financial Well-Being in America (2017)",
              "Monarch Butterfly Habitat Restoration: Polygon (Feature Layer)",
              "Adult Depression (LGHC Indicator)",
              "Feed Grains Database",
              "Normal weight, overweight, and obesity among adults aged 20 and over, by selected characteristics: United States",
              "Provisional COVID-19 Deaths by HHS Region, Race, and Age",
              "COVID-19 Patient Data",
              "NYPD Arrest Data (Year to Date)",
              "Monthly Counts of Deaths by Select Causes, 2014-2019",
              "Accidental Drug Related Deaths 2012-2022",
              "Grocery Stores",
              "Rates and Trends in Heart Disease and Stroke Mortality Among US Adults (35+) by County, Age Group, Race/Ethnicity, and Sex – 2000-2019",
              "Drug Use Data from Selected Hospitals",
              "Alternative Fueling Station Locations",
              "Clean Air Status and Trends Network (CASTNET): Ozone",
              "Employee Payroll",
              "Real Estate Across the United States (REXUS) Inventory (Building)",
              "Zoning by Address",
              "SBS Certified Business List",
              "Electric Vehicle Title and Registration Activity",
              "Retail Food Stores",
              "Behavioral Risk Factor Data: Tobacco Use (2011 to present)",
              "NYPD Arrests Data (Historic)",
              "COVID-19 Daily Counts of Cases, Hospitalizations, and Deaths",
              "National Health Interview Survey (NHIS) - National Cardiovascular Disease Surveillance Data",
              "Motor Carrier Registrations - Census Files",
              "Water Quality Data",
              "2012 SAT Results",
              "2010 Census Populations by Zip Code",
              "Nutrition, Physical Activity, and Obesity - American Community Survey",
              "Infectious Diseases by Disease, County, Year, and Sex",
              "1.08 Crash Data Report (detail)",
              "Medicare Monthly Enrollment",
              "Harmonized Tariff Schedule of the United States (2023)",
              "Average Daily Traffic Counts",
              "School Neighborhood Poverty Estimates - Current",
              "2021 Yellow Taxi Trip Data",
              "Heart Disease Mortality Data Among US Adults (35+) by State/Territory and County – 2018-2020",
              "FHFA House Price Indexes (HPIs)",
              "NYPD Hate Crimes",
              "Power Outages - Zipcode",
              "Sales Tax Collections by State",
              "Post-COVID Conditions",
              "Youth Tobacco Survey (YTS) Data",
              "Sex Offenders",
              "Fire Department Calls for Service",
              "Recalls Data",
              "Greenhouse Gas Emissions",
              "Crime Data from 2010 to 2019",
              "Current Employment Statistics (CES)",
              "Chemicals in Cosmetics",
              "Center for Medicare & Medicaid Services (CMS) , Medicare Claims data",
              "Biodiversity by County - Distribution of Animals, Plants and Natural Communities",
              "Solar Footprints in California",
              "Sugar-Sweetened Beverage Consumption in California Residents",
              "COVID-19 Case Surveillance Public Use Data",
              "Trips by Distance",
              "National Survey of Family Growth",
              "Electric Vehicle Population Size History",
              "Total tonnage of commodites carried on commercial waterways by traffic type",
              "Youth Behavior Risk Survey (High School)",
              "MVA Vehicle Sales Counts by Month for Calendar Year 2002 through June 2023",
              "Licensed Drivers, by state, gender, and age group",
              "Employee Travel Data (Non-Local)",
              "Estimated Gasoline Sales: Beginning 1995",
              "Weekly Provisional Counts of Deaths by State and Select Causes, 2020-2023",
              "Demographics by Zip Code",
              "Tax Lien Sale Lists",
              "Violence Reduction - Victims of Homicides and Non-Fatal Shootings",
              "Part 1 Crime Data",
              "FS National Forests Dataset (US Forest Service Proclaimed Forests)",
              "Cannabis Retail Sales by Week Ending",
              "COVID-19 Cases, Tests, and Deaths by ZIP Code",
              "Index, Violent, Property, and Firearm Rates By County: Beginning 1990",
              "U.S. Electric Utility Companies and Rates: Look-up by Zipcode (2021)",
              "Hospitalization Discharge Rates",
              "Pittsburgh American Community Survey 2014 - Miscellaneous Data",
              "Allegheny County Tax Liens (Filings, Satisfactions, and Current Status)",
              "Air Traffic Landings Statistics",
              "Clean Air Status and Trends Network (CASTNET) Download Data Module",
              "Proportion of Adults Who Are Current Smokers (LGHC Indicator)",
              "Summer Sports Experience",
              "Good Food Purchasing Data",
              "Affordable Housing by Town 2011-2022",
              "Unintentional Drug Overdose Death Rate by Race/Ethnicity",
              "Global Landslide Catalog Export",
              "Uranium Location Database",
              "Behavioral Risk Factor Surveillance System (BRFSS) -  National Cardiovascular Disease Surveillance Data",
              "Baltimore City Employee Salaries",
              "Atlas of Rural and Small-Town America",
              "311 Data",
              "Telemedicine Use in the Last 4 Weeks",
              "COVID-19 Hospital Data",
              "5.12 Cybersecurity (detail)",
              "Street Names",
              "Crash Data",
              "State Energy Data System (SEDS)",
              "National Greenhouse Gas Emission Inventory",
              "College Credit Card Marketing Agreements Data",
              "U.S. Life Expectancy at Birth by State and Census Tract - 2010-2015",
              "NYC Taxi Zones",
              "Violence Reduction - Victim Demographics - Aggregated",
              "Monthly Casino Slot Revenue for Current Year",
              "Citywide Payroll Data (Fiscal Year)",
              "DIR Electrician Certification Unit (ECU)",
              "Nutrition, Physical Activity, and Obesity - Women, Infant, and Child",
              "TSCA Inventory",
              "Iowa Liquor Sales",
              "Bedbug Reporting",
              "Water Consumption And Cost (2013 - Feb 2023)",
              "2013 - 2015 New York State Mathematics Exam",
              "Motor Vehicle Collisions - Vehicles",
              "Vacation Rentals (Hotels, B&B, short-term rentals, etc.)",
              "Financial Institutions",
              "State of Oregon Social Media Sites",
              "My Brother's Keeper Key Statistical Indicators on Boys and Men of Color",
              "2020 Presidential General Election Results",
              "911 Calls for Service 2021",
              "Pregnancy-Associated Mortality",
              "Spill Incidents",
              "Vehicle Fuel Type Count by Zip Code",
              "Public School Locations 2021-22",
              "National Vital Statistics System (NVSS) - National Cardiovascular Disease Surveillance Data",
              "Report on U.S. Methane Emissions 1990-2020: Inventories, Projections, and Opportunities for Reductions: 2001 Updated emission and cost estimates",
              "High School Electronic Smoking Device and Tobacco Use Prevalence",
              "Authorizations From 10/01/2006 Thru 3/31/2023",
              "Motor Vehicle Registrations Dashboard data",
              "Adult Cigarette and Tobacco Use Prevalence",
              "Liquor Brands",
              "Health Care Cost Growth",
              "Public School Characteristics 2020-21",
              "COVID-19 Reported Patient Impact and Hospital Capacity by Facility -- RAW",
              "Police Department Incident Reports: 2018 to Present",
              "Surgical Site Infections (SSIs) for Operative Procedures in California Hospitals",
              "AH Provisional Diabetes Death Counts for 2020",
              "FAIN",
              "Subway Stations",
              "Childhood Asthma Healthcare Utilization",
              "DOHMH Dog Bite Data",
              "Local Area Unemployment Statistics (LAUS)",
              "AH Monthly Provisional Counts of Deaths for Select Causes of Death by Sex, Age, and Race and Hispanic Origin",
              "Global Landslide Catalog",
              "Hospital Encounters for Behavioral Health",
              "Death Probabilities for Females (2011-2090)",
              "Medicare Part D Spending by Drug",
              "PLACES: ZCTA Data (GIS Friendly Format), 2023 release",
              "Certified Disadvantaged Business Enterprise (DBE) Directory",
              "NIJ's Recidivism Challenge Full Dataset",
              "Overdose-Related 911 Responses by Emergency Medical Services",
              "Occupational Employment and Wage Statistics",
              "Alien Plant Threat Assessment",
              "Animal Control Incidents",
              "Annual Crime Dataset 2015",
              "National Obesity By State",
              "Autonomous Vehicle Survey of Bicyclists and Pedestrians in Pittsburgh",
              "Hospital Provider Cost Report",
              "Number of Cancer Surgeries (Volume) Performed in California Hospitals",
              "Hourly Energy Emission Factors for Electricity Generation in the United States",
              "Next Generation Simulation (NGSIM) Vehicle Trajectories and Supporting Data",
              "Home Mortgage Disclosure Act (HMDA) Public Data from 2007-2017",
              "Restaurant and Market Health Inspections",
              "Postsecondary School Locations - Current",
              "autism prevalence studies",
              "Residential Energy Consumption Survey (RECS) Files, Energy Consumption, 2009",
              "Fruit and Vegetable Consumption in California Residents, 2012/2013",
              "Weekly United States COVID-19 Cases and Deaths among Dialysis Patients - ARCHIVED",
              "Racial and Social Equity Composite Index Current",
              "Agency Identifier",
              "Lost, found, adoptable pets",
              "Total tonnage (foreign and domestic) of commodites carried on commercial waterways",
              "Pregnancy Risk Assessment Monitoring System (PRAMS)",
              "HSR Merger Filings by Month",
              "DOHMH New York City Restaurant Inspection Results",
              "Selected Online Sport Wagering Data",
              "Provisional COVID-19 Deaths by Sex and Age",
              "Loss of Work Due to Illness from COVID-19",
              "Microplastics in Drinking Water",
              "COVID-19 Vaccinations by Age Group - ARCHIVED",
              "Local Weather Archive",
              "Photovoltaic Data Acquisition (PVDAQ) Public Datasets",
              "Suicide, Deaths per 100,000 Population (LGHC Indicator)",
              "Child Mental Health Treatment",
              "Insurance Company Complaints, Resolutions, Status, and Recoveries",
              "Producer Price Index",
              "ABS Store Inventory and Sale Items",
              "HealthCare.gov Transitions Marketplace Medicaid Unwinding Report",
              "Arrests",
              "New York City Leading Causes of Death",
              "Dataset inventory",
              "Crime",
              "Prescription Monitoring Program (PMP) Public Use Data",
              "Key Economic Indicators",
              "2018 Central Park Squirrel Census - Squirrel Data",
              "Zip Code Lookup Table",
              "Employee Compensation",
              "Website Analytics",
              "NYC Dog Licensing Dataset",
              "UNSPSC Codes",
              "Adult Mental Health Treatment",
              "PLACES: County Data (GIS Friendly Format), 2020 release",
              "Monthly Provisional Counts of Deaths by Select Causes, 2020-2023",
              "Product Data for Newly Reported Drugs in the Medicaid Drug Rebate Program 2023-10-02-to-2023-10-08",
              "ODF Fire Occurrence Data 2000-2022",
              "Motor Vehicle Use Map: Roads (Feature Layer)",
              "Vaccines.gov: COVID-19 vaccinating provider locations",
              "Allegheny County Assets",
              "2019 NYC School Survey - Student",
              "Daily Transit Ridership",
              "Medicare Physician & Other Practitioners - by Provider",
              "Recycling Diversion and Capture Rates",
              "CDPH Mental Health Resources",
              "Rates and Trends in Coronary Heart Disease and Stroke Mortality Data Among US Adults (35+) by County – 1999-2018",
              "2020 Green Taxi Trip Data",
              "Influenza Vaccination Coverage for All Ages (6+ Months)",
              "All-Cause Unplanned 30-Day Hospital Readmission Rate, California (LGHC Indicator)",
              "U.S. State Life Expectancy by Sex, 2020",
              "Clean Air Markets Program Data Application (CAMPD)",
              "Integrated Compliance Information System (ICIS)",
              "Access to Intercity Air, Bus, and Rail Transportation in Rural Areas",
              "2019 Walk & Bike Count Data",
              "LA County COVID Cases",
              "Toxic Substances Control Act (TSCA) 8(e) Notices and FYI Submissions",
              "Gasoline Retail Prices Weekly Average by Region: Beginning 2007",
              "Global Non-CO2 Greenhouse Gas Emission Projections & Mitigation: 2015-2050",
              "Police Department Stop Data",
              "COVID-19 Cases and Deaths by Race/Ethnicity - ARCHIVE",
              "Traffic Crashes Resulting in Injury",
              "National Adult Tobacco Survey (NATS)",
              "Current Employee Names, Salaries, and Position Titles",
              "WA Tax Exemptions - Potential Eligibility by Make/Model Excluding Vehicle Price Criteria",
              "Air Traffic Cargo Statistics",
              "Taxi Trips",
              "Demographics",
              "Water Quality",
              "Nutrition, Physical Activity, and Obesity - Youth Risk Behavior Surveillance System",
              "Diabetes & Hypertension & Hyperlipidemia comorbidity",
              "EPA Enforcement and Compliance History Online"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Datasets",
            "description_tooltip": null,
            "disabled": false,
            "index": 64,
            "layout": "IPY_MODEL_58bddeafd3b049baad94041858e3e577",
            "style": "IPY_MODEL_53af2bfa3ce141609da2f5843f88602b"
          }
        },
        "58bddeafd3b049baad94041858e3e577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "53af2bfa3ce141609da2f5843f88602b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathewsrc/Streamlined-ETL-Process-Unleashing-Polars-Dataprep-and-Airflow/blob/master/notebooks/gov_etl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlined ETL Process: Unleashing Polars and Dataprep"
      ],
      "metadata": {
        "id": "xbeyh2FWetc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Summary:**\n",
        "\n",
        "This ETL (Extract, Transform, Load) project employs several Python libraries, including Polars, Dataprep, Requests, BeautifulSoup, and Loguru, to streamline the extraction, transformation, and loading of CSV datasets from the U.S. government's data repository at https://catalog.data.gov.\n",
        "\n"
      ],
      "metadata": {
        "id": "XQneUg0fClmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Objectives:**\n",
        "\n",
        "Extraction: I utilize the requests library and BeautifulSoup to scrape datasets from https://catalog.data.gov, a repository of various data formats, including CSV, XLS, and HTML.\n",
        "\n",
        "Transformation: Data manipulation and cleaning are accomplished using Polars, a high-performance data manipulation library written in Rust.\n",
        "\n",
        "Data Profiling: Dataprep is employed to create dynamic data reports and facilitate data profiling, quality assessment, and visualization, providing insights into data quality and characteristics.\n",
        "\n",
        "Loading: Transformed data is saved in CSV files using Polars.\n",
        "\n",
        "Logging: Loguru is chosen for logging, ensuring transparency and facilitating debugging throughout the ETL process.\n",
        "\n",
        "Through the automation of these ETL tasks, I establish a robust data pipeline that transforms raw data into valuable assets, supporting informed decision-making and data-driven insights."
      ],
      "metadata": {
        "id": "qCxLL3DWAGLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directory structure**\n",
        "\n",
        "- **data/raw/**: Contains raw data files.\n",
        "- **data/interim/**: Contains intermediary processed data.\n",
        "- **data/processed/**: Contains processed data.\n",
        "- **data/reports/**: Contains reports for each dataset."
      ],
      "metadata": {
        "id": "EkWfyHr68Igk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "Ez3Aqqq3Vs7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q loguru==0.7.2\n",
        "!pip install -q polars==0.19.7\n",
        "!pip install -q furl==2.1.3\n",
        "!pip install -q tqdm==4.66.1\n",
        "!pip install -q dataprep==0.4.5\n",
        "!pip install -q requests==2.31.0\n",
        "!pip install -q pandas==1.5.3 # pandas 2.1.1 does not work with dataprep current version\n",
        "!pip install -q beautifulsoup4==4.11.2"
      ],
      "metadata": {
        "id": "dXYtgXthx9Jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86190a0-df5e-42d9-d81b-0463f9fe41b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.6/133.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.0/764.0 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for metaphone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.3.24 which is incompatible.\n",
            "panel 1.2.3 requires bokeh<3.3.0,>=3.1.1, but you have bokeh 2.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "5zJz-MTyVvwq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RkPBYbeGNFtn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import furl\n",
        "import polars as pl\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "from loguru import logger\n",
        "import sys\n",
        "import re\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import dataprep\n",
        "from dataprep.eda import create_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Filter out warnings throwed by dataprep\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "metadata": {
        "id": "9AZzkLhbY2km"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print package versions\n",
        "print(f\"requests version: {requests.__version__}\") #2.31.0\n",
        "print(f\"furl version: {furl.__version__}\") #2.1.3\n",
        "print(f\"polars version: {pl.__version__}\") #0.19.7\n",
        "print(f\"BeautifulSoup version: {bs4.__version__}\") #4.11.2\n",
        "print(f\"pandas version: {pd.__version__}\") #1.5.3\n",
        "print(f\"Python version: {sys.version}\")"
      ],
      "metadata": {
        "id": "0DqiqrW3xOT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9f8516-99ba-4c6f-aeef-ca1cc3681552"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requests version: 2.31.0\n",
            "furl version: 2.1.3\n",
            "polars version: 0.19.7\n",
            "BeautifulSoup version: 4.11.2\n",
            "pandas version: 1.5.3\n",
            "Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create directories to save data"
      ],
      "metadata": {
        "id": "8-zSeK8sC464"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directories to save data\n",
        "\n",
        "raw_dir = \"./data/raw\"\n",
        "interim_dir = \"./data/interim\"\n",
        "processed_dir = \"./data/processed\"\n",
        "reports_dir = \"./data/reports\"\n",
        "\n",
        "# Create directories\n",
        "for dir in [raw_dir, interim_dir, processed_dir, reports_dir]:\n",
        "    os.makedirs(dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "lj5YxgIm6vWv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define files time period to improve organization"
      ],
      "metadata": {
        "id": "-pAojves7f5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the current date and time\n",
        "current_datetime = datetime.now()\n",
        "\n",
        "# Format the current date and time to obtain the time period as \"YYYY-MM\"\n",
        "time_period = current_datetime.strftime(\"%Y_%m\")\n",
        "logger.info(f\"Time period: {time_period}\")"
      ],
      "metadata": {
        "id": "Z92dUv__nFqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa62b3e7-86dd-4470-947b-179db30e210b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-16 16:02:26.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 6>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mTime period: 2023_10\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract"
      ],
      "metadata": {
        "id": "_vOPFIqJDb7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "@logger.catch\n",
        "def format_url(url):\n",
        "    \"\"\"\n",
        "    Format URL\n",
        "\n",
        "    Args:\n",
        "        url (str): The original URL to be formatted.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted URL.\n",
        "\n",
        "    Example:\n",
        "        >>> format_url(\"https://www.ers.usda.gov/page?q=example\")\n",
        "        'https://www.ers.usda.gov/page'\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Define a regular expression pattern to remove query parameters from the URL\n",
        "    pattern = r'\\?.+'\n",
        "\n",
        "    # Use regular expression substitution to remove the pattern from the URL\n",
        "    url = re.sub(pattern, '', url)\n",
        "\n",
        "     # Define a prefix for the URL\n",
        "    prefix = \"https://www.ers.usda.gov\"\n",
        "\n",
        "    # Add prefix if URL does not start with https:// or http://\n",
        "    if not url.startswith(\"https://\") and not url.startswith(\"http://\"):\n",
        "        url = furl.furl(prefix).add(path=url).url\n",
        "\n",
        "    return url"
      ],
      "metadata": {
        "id": "bb9k_JSp1-pq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assert that `format_url()` is working as expected\n"
      ],
      "metadata": {
        "id": "Mux2fn252knk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function with assertions\n",
        "assert format_url(\"https://www.ers.usda.gov/page?q=example\") == 'https://www.ers.usda.gov/page'\n",
        "assert format_url(\"https://www.ers.usda.gov/\") == 'https://www.ers.usda.gov/'\n",
        "assert format_url(\"/webdocs/DataFiles/106595/Dates2020.csv?v=5007.3\") == \"https://www.ers.usda.gov/webdocs/DataFiles/106595/Dates2020.csv\"\n",
        "assert format_url(\"/webdocs/DataFiles/51035/FruitPrices2020.csv?v=5007\") == \"https://www.ers.usda.gov/webdocs/DataFiles/51035/FruitPrices2020.csv\"\n",
        "assert format_url(\"https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD\") == \"https://data.wa.gov/api/views/f6w7-q2d2/rows.csv\""
      ],
      "metadata": {
        "id": "ZZanaEq5D-4f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "@logger.catch\n",
        "def modify_file_name(name):\n",
        "    \"\"\"\n",
        "    Modify a file name by performing the following transformations:\n",
        "\n",
        "    Args:\n",
        "        name (str): The original file name to be modified.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified file name.\n",
        "\n",
        "    Example:\n",
        "        >>> modify_file_name(\"My File 123.txt\")\n",
        "        'my_file_123.txt'\n",
        "\n",
        "        >>> modify_file_name(\"!@#File Name$%^\")\n",
        "        'file_name'\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the file name to lowercase\n",
        "    name = name.lower()\n",
        "\n",
        "    # Replace non-alphanumeric and non-underscore characters with underscores\n",
        "    name = re.sub(r'[^a-zA-Z0-9_\\.]+', '_', name)\n",
        "\n",
        "    # Remove non-alphanumeric characters from the start and end of the name\n",
        "    name = re.sub(r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9]+$', '', name)\n",
        "\n",
        "    return name\n"
      ],
      "metadata": {
        "id": "6pRBf0s9CVe_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert modify_file_name(\"My File 123.txt\") == \"my_file_123.txt\"\n",
        "assert modify_file_name(\"!@#File Name$%^\") == \"file_name\"\n",
        "assert modify_file_name(\"File123\") == \"file123\"\n",
        "assert modify_file_name(\"   leading_trailing   \") == \"leading_trailing\"\n",
        "assert modify_file_name(\"_#Name_With_Underscores_%\") == \"name_with_underscores\""
      ],
      "metadata": {
        "id": "2f4h5_j5Chs1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@logger.catch\n",
        "def extract(url, name):\n",
        "    \"\"\"\n",
        "    Extract data from a specified URL, format the URL and file name,\n",
        "    and save the data to a file.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the data source.\n",
        "        name (str): The name of the data item.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the saved data file, or None if extraction fails.\n",
        "\n",
        "    Example:\n",
        "        >>> extract(\"https://www.example.com/data.csv\", \"Sample Data\")\n",
        "        'path_to_saved_data/Sample_Data.csv'\n",
        "\n",
        "        >>> extract(\"https://www.example.com/data.txt\", \"Text Data\")\n",
        "        # Error: Unsupported file format\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\")\n",
        "    logger.info(\"EXTRACT STARTED!\")\n",
        "\n",
        "    # Log the URL and name of the selected item\n",
        "    logger.info(f'Selected item URL: {url}')\n",
        "    logger.info(f'Selected item name: {name}')\n",
        "\n",
        "    # Format the file name\n",
        "    name = modify_file_name(name)\n",
        "\n",
        "    # Format the URL\n",
        "    url = format_url(url)\n",
        "\n",
        "    # Log the formatted URL and name\n",
        "    logger.info(f'Selected item URL (Formatted): {url}')\n",
        "    logger.info(f'Selected item name (Formatted): {name}')\n",
        "\n",
        "    # Determine the file extension from the URL\n",
        "    file_extension = url.split('.')[-1].lower()\n",
        "\n",
        "    # Generate the output file name\n",
        "    filename = f\"{name}_{time_period}.{file_extension}\"\n",
        "\n",
        "    # Handle CSV files\n",
        "    if file_extension == 'csv':\n",
        "        output_path = os.path.join(raw_dir, filename)\n",
        "        pl.read_csv(url, try_parse_dates=True, ignore_errors=True).write_csv(output_path)\n",
        "    else:\n",
        "        logger.error(f\"Unsupported file format: {file_extension}\")\n",
        "        return None\n",
        "\n",
        "    # Log success and return the path to the saved file\n",
        "    logger.success(f\"Data successfully fetched and saved at {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "pO54eEbDNwGi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if `extract()` function is working as expected"
      ],
      "metadata": {
        "id": "nT3FY1vPDfRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD\"\n",
        "expected_output_file = f\"electric_vehicle_population_data_{time_period}.csv\"\n",
        "expected_output_path = os.path.join(raw_dir, expected_output_file)\n",
        "extracted_output_path =  extract(url, \"Electric Vehicle Population Data\")\n",
        "assert extracted_output_path == expected_output_path"
      ],
      "metadata": {
        "id": "EWQo6T1Z9BSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97dda64-2b88-4048-c611-9a5791ac53a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-16 16:02:26.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEXTRACT STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-16 16:02:26.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mSelected item URL: https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD\u001b[0m\n",
            "\u001b[32m2023-10-16 16:02:26.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mSelected item name: Electric Vehicle Population Data\u001b[0m\n",
            "\u001b[32m2023-10-16 16:02:26.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mSelected item URL (Formatted): https://data.wa.gov/api/views/f6w7-q2d2/rows.csv\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-16 16:02:26.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mSelected item name (Formatted): electric_vehicle_population_data\u001b[0m\n",
            "\u001b[32m2023-10-16 16:03:01.933\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m54\u001b[0m - \u001b[32m\u001b[1mData successfully fetched and saved at ./data/raw/electric_vehicle_population_data_2023_10.csv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform"
      ],
      "metadata": {
        "id": "R8n2vQqKDnwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "@logger.catch\n",
        "def lower_column(df):\n",
        "    \"\"\"\n",
        "    Rename columns to lowercase in a Polars DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pl.DataFrame): The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: The Polars DataFrame with column names in lowercase.\n",
        "\n",
        "    Example:\n",
        "        >>> df = pl.DataFrame({\n",
        "        ...     \"Column1\": [1, 2, 3],\n",
        "        ...     \"Column2\": [\"A\", \"B\", \"C\"]\n",
        "        ... })\n",
        "        >>> df = lower_column(df)\n",
        "        >>> print(df)\n",
        "        shape: (3, 2)\n",
        "        ┌────────┬────────┐\n",
        "        │ column1│ column2│\n",
        "        │ int    │ str    │\n",
        "        ╞════════╪════════╡\n",
        "        │ 1      │ \"A\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 2      │ \"B\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 3      │ \"C\"    │\n",
        "        └────────┴────────┘\n",
        "\n",
        "    Note:\n",
        "        This function renames columns in the input Polars DataFrame to lowercase.\n",
        "        It creates a new DataFrame with column names converted to lowercase while keeping the original data intact.\n",
        "\n",
        "    \"\"\"\n",
        "    return df.select([pl.col(col).alias(col.lower().replace(\" \", \"_\")) for col in df.columns])"
      ],
      "metadata": {
        "id": "mhMT9GcvqbWN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "@logger.catch\n",
        "def drop_duplicates(df):\n",
        "    \"\"\"\n",
        "    Remove duplicate rows from a Polars DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pl.DataFrame): The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: The Polars DataFrame with duplicate rows removed.\n",
        "\n",
        "    Example:\n",
        "        >>> df = pl.DataFrame({\n",
        "        ...     \"Column1\": [1, 2, 2, 3, 4],\n",
        "        ...     \"Column2\": [\"A\", \"B\", \"B\", \"C\", \"D\"]\n",
        "        ... })\n",
        "        >>> df = drop_duplicates(df)\n",
        "        >>> print(df)\n",
        "        shape: (4, 2)\n",
        "        ┌────────┬────────┐\n",
        "        │ column1│ column2│\n",
        "        │ int    │ str    │\n",
        "        ╞════════╪════════╡\n",
        "        │ 1      │ \"A\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 2      │ \"B\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 3      │ \"C\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 4      │ \"D\"    │\n",
        "        └────────┴────────┘\n",
        "\n",
        "    Note:\n",
        "        This function removes duplicate rows from the input Polars DataFrame.\n",
        "        It creates a new DataFrame with duplicate rows removed, but it does not modify the original DataFrame.\n",
        "\n",
        "    \"\"\"\n",
        "    return df.unique(keep=\"first\")"
      ],
      "metadata": {
        "id": "U15jenvIqe4n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "def drop_full_row_null(df):\n",
        "    \"\"\"\n",
        "    Drop rows from a polars DataFrame if all values in a row are null.\n",
        "\n",
        "    Args:\n",
        "        df (pl.DataFrame): The input polars DataFrame from which rows with all-null values will be removed.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: A new polars DataFrame with rows removed if all values in the row are null.\n",
        "\n",
        "    Example:\n",
        "        >>> import polars as pl\n",
        "        >>> df = pl.DataFrame({\n",
        "        ...     'A': [1, 2, None, 4],\n",
        "        ...     'B': [None, None, None, None]\n",
        "        ... })\n",
        "        >>> result = drop_full_row_null(df)\n",
        "        >>> print(result)\n",
        "        shape: (3, 2)\n",
        "        ┌─────┬─────┐\n",
        "        │ A   │ B   │\n",
        "        │ --- │ --- │\n",
        "        │ i64 │ i64 │\n",
        "        ╞═════╪═════╡\n",
        "        │ 1   │ null│\n",
        "        ├─────┼─────┤\n",
        "        │ 2   │ null│\n",
        "        ├─────┼─────┤\n",
        "        │ 4   │ null│\n",
        "        └─────┴─────┘\n",
        "    \"\"\"\n",
        "    return df.filter(~pl.all_horizontal(pl.all().is_null()))\n"
      ],
      "metadata": {
        "id": "N--St4QLqiq-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "@logger.catch\n",
        "def transform(input_path, name):\n",
        "    \"\"\"\n",
        "    Transform a CSV file by converting all text columns to lowercase and save the result.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The path to the input CSV file to be transformed.\n",
        "        name (str): A descriptive name for the transformation process.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the transformed CSV file.\n",
        "\n",
        "    Example:\n",
        "        >>> transform(\"input_data.csv\", \"Lowercase Conversion\")\n",
        "        'path_to_transformed_data/input_data.csv'\n",
        "    \"\"\"\n",
        "    logger.info(\"TRANSFORM STARTED!\")\n",
        "\n",
        "    # Determine the file extension from the filename\n",
        "    file_extension = input_path.split('.')[-1].lower()\n",
        "\n",
        "\n",
        "    # Extract the file name from the path\n",
        "    file_name = os.path.basename(input_path)\n",
        "\n",
        "    # Define the output file path\n",
        "    output_path = os.path.join(interim_dir, file_name)\n",
        "\n",
        "    # Read the CSV file as LazyFrame using polars\n",
        "    df = pl.scan_csv(input_path)\n",
        "\n",
        "    df = (df.pipe(lower_column)  # Convert all text columns to lowercase\n",
        "            .pipe(drop_duplicates)   # Drop duplicate rows\n",
        "            .pipe(drop_full_row_null)  # Drop a row only if all values are null\n",
        "    )\n",
        "\n",
        "    # Save the transformed data as a CSV file\n",
        "    df.collect().write_csv(output_path)\n",
        "\n",
        "    # Log success and return the path to the transformed file\n",
        "    logger.success(f\"Data successfully transformed and saved at: {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "1EEhG8TVOobk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if `transform()` function is working as expected"
      ],
      "metadata": {
        "id": "NFclR4vADxKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expect_transformed_output_path = './data/interim/electric_vehicle_population_data_2023_10.csv'\n",
        "name = f\"electric_vehicle_population_data_{time_period}.csv\"\n",
        "input_path = os.path.join(raw_dir, expected_output_file)\n",
        "transformed_output_path = transform(input_path, name)\n",
        "assert os.path.exists(expect_transformed_output_path)\n",
        "assert transformed_output_path == expect_transformed_output_path"
      ],
      "metadata": {
        "id": "84U1U2lTPLyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bca41e1-2736-4072-fb5d-f21805f9b921"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-16 16:03:01.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtransform\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mTRANSFORM STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-16 16:03:02.485\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtransform\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1mData successfully transformed and saved at: ./data/interim/electric_vehicle_population_data_2023_10.csv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load"
      ],
      "metadata": {
        "id": "jHvYXeE3D38g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@logger.catch\n",
        "def load(input_path, name, report=True):\n",
        "    \"\"\"\n",
        "    Load data from a CSV file and save it to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The path to the input CSV file to be loaded.\n",
        "        name (str): A descriptive name for the loading process.\n",
        "        report (bool, optional): If True, a report will be created and saved. Default is True.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the loaded CSV file.\n",
        "\n",
        "    Example:\n",
        "        >>> load(\"input_data.csv\", \"Data Loading\")\n",
        "        'path_to_processed_data/input_data.csv'\n",
        "    \"\"\"\n",
        "    logger.info(\"LOAD STARTED!\")\n",
        "\n",
        "    # Determine the file format based on the filename extension\n",
        "    file_extension = input_path.split('.')[-1].lower()\n",
        "\n",
        "    # Extract the file name from the path\n",
        "    file_name = os.path.basename(input_path)\n",
        "\n",
        "    # Define the output file path in the processed directory\n",
        "    output_path = os.path.join(processed_dir, file_name)\n",
        "\n",
        "    # Read the CSV file using polars and save it to the processed directory\n",
        "    df = pl.read_csv(input_path)\n",
        "\n",
        "    if report:\n",
        "        # Create and save report\n",
        "        create_report(df.to_pandas(), progress=False).save(os.path.join(\n",
        "            reports_dir, f\"{file_name.split('.')[0]}_report.html\"))\n",
        "        logger.success(\"Report successfully created\")\n",
        "\n",
        "    df.write_csv(output_path)\n",
        "\n",
        "    # Log success and return the path to the loaded file\n",
        "    logger.success(f\"Data successfully loaded at {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "hBeBX9QlOV9-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if `load()` function is working as expected"
      ],
      "metadata": {
        "id": "0fP0YgtXD57Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expect_loaded_output_path = './data/processed/electric_vehicle_population_data_2023_10.csv'\n",
        "input_path = './data/interim/electric_vehicle_population_data_2023_10.csv'\n",
        "name = f\"electric_vehicle_population_data_{time_period}.csv\"\n",
        "loaded_output_path = load(input_path, name, False)\n",
        "assert  os.path.exists(expect_loaded_output_path)\n",
        "assert  loaded_output_path == expect_loaded_output_path\n",
        "\n",
        "# Delete created files by assert\n",
        "os.remove('./data/processed/electric_vehicle_population_data_2023_10.csv')\n",
        "os.remove('./data/interim/electric_vehicle_population_data_2023_10.csv')\n",
        "os.remove('./data/raw/electric_vehicle_population_data_2023_10.csv')"
      ],
      "metadata": {
        "id": "ptmtJswVSeyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c045111-e51d-4591-f447-6aa53cedc4be"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-16 16:03:02.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLOAD STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-16 16:03:02.917\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m41\u001b[0m - \u001b[32m\u001b[1mData successfully loaded at ./data/processed/electric_vehicle_population_data_2023_10.csv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all together"
      ],
      "metadata": {
        "id": "WzQj_WCuVmcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "@logger.catch\n",
        "def etl_workflow(tagged_urls):\n",
        "    \"\"\"\n",
        "    Perform Extract, Transform, and Load (ETL) process for a list of tagged URLs.\n",
        "\n",
        "    Args:\n",
        "        tagged_urls (list): A list of tuples, each containing a dataset name and its URL.\n",
        "\n",
        "    The function iterates through the tagged URLs, extracts data, applies transformation,\n",
        "    and loads the data. The ETL process is performed for each dataset in the list.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    for name, url in tqdm(tagged_urls):\n",
        "        output_path = extract(url, name)\n",
        "        if output_path is None:\n",
        "            continue\n",
        "        output_path = transform(output_path, name)\n",
        "        if output_path is None:\n",
        "            continue\n",
        "        load(output_path, name)\n",
        "\n",
        "    logger.success(\"ETL COMPLETED!\")"
      ],
      "metadata": {
        "id": "i56QmgZ7ycoz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fetch datasets from\n",
        "\n",
        "https://catalog.data.gov/dataset/"
      ],
      "metadata": {
        "id": "rxipnksUS-sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "\n",
        "@logger.catch\n",
        "def fetch_datasets(number_of_pages=3):\n",
        "    \"\"\"\n",
        "    Fetch datasets from data.gov catalog web pages and save them to a Parquet file.\n",
        "\n",
        "    Args:\n",
        "        number_of_pages (int, optional): The number of catalog pages to scrape. Default is 3.\n",
        "\n",
        "    This function scrapes dataset information from data.gov catalog web pages,\n",
        "    extracts dataset names, organizations, descriptions, and CSV links to dataset\n",
        "    resources, and then saves the data as a Parquet file.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "\n",
        "    for page in tqdm(range(1, number_of_pages + 1)):\n",
        "        url = f\"https://catalog.data.gov/dataset/?page={page}\"\n",
        "\n",
        "        # Send an HTTP GET request to the specified URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the response status code is 200 (OK)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the web page using BeautifulSoup\n",
        "            bs = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            ul_tag = bs.find(\"ul\", class_=\"dataset-list unstyled\")\n",
        "\n",
        "            if ul_tag:\n",
        "                li_tags = bs.find_all(\"li\", class_=\"dataset-item has-organization\")\n",
        "                for li in li_tags:\n",
        "                    # Extract the dataset name\n",
        "                    name = li.find('h3').find('a').text\n",
        "\n",
        "                    # Extract the dataset organization\n",
        "                    organization = li.find('p', class_='dataset-organization').text.replace(\"—\", \"\")\n",
        "\n",
        "                    # Extract the dataset description\n",
        "                    description = li.find('div', class_='notes').find('div').text\n",
        "\n",
        "                    # Extract CSV links to dataset resources\n",
        "                    links = [a['href'] for a in li.find_all('a', class_='label label-default', attrs={\"data-format\": \"csv\"})]\n",
        "\n",
        "                    # Add dataset if it contains links\n",
        "                    if len(links) > 0:\n",
        "                        datasets.append({\n",
        "                            \"name\": name,\n",
        "                            \"organization\": organization,\n",
        "                            \"description\": description,\n",
        "                            \"links\": links\n",
        "                        })\n",
        "        else:\n",
        "            logger.error(\"Failed to fetch the web page.\")\n",
        "\n",
        "    # Save the collected dataset information as a Parquet file\n",
        "    pl.LazyFrame(datasets).sink_parquet(\"datasets.parquet\")\n",
        "\n",
        "    logger.success(\"DataFrame saved as ./datasets.parquet\")\n"
      ],
      "metadata": {
        "id": "4UqrMcg-S-F1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from loguru import logger\n",
        "\n",
        "@logger.catch\n",
        "def on_dropdown_change(change):\n",
        "    \"\"\"\n",
        "    Callback function triggered when the dropdown value changes.\n",
        "\n",
        "    Args:\n",
        "        change (ipywidgets.widgets.widget.CallbackDispatcher.ChangeTrait): The change event.\n",
        "\n",
        "    This function is called when the value of the dropdown widget changes. It checks if\n",
        "    the new value is empty and, if not, initiates the ETL (Extract, Transform, Load) workflow\n",
        "    for the selected dataset.\n",
        "\n",
        "    Args:\n",
        "        change (ipywidgets.widgets.widget.CallbackDispatcher.ChangeTrait): The change event object.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    selected_option = change.owner.options[change.owner.index]\n",
        "\n",
        "    # Check if the new value is empty\n",
        "    if change.new == \"\":\n",
        "        return\n",
        "\n",
        "    name, urls = selected_option[0], change.new\n",
        "\n",
        "    for url in urls:\n",
        "        etl_workflow([(name, url)])"
      ],
      "metadata": {
        "id": "KUBq4JZGma0K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "@logger.catch\n",
        "def show_datasets():\n",
        "    \"\"\"\n",
        "    Display a dropdown widget containing a list of datasets loaded from a Parquet file.\n",
        "\n",
        "    This function loads a Parquet dataset, removes rows with null values, creates a list of items\n",
        "    (dataset name, links) from DataFrame rows, and displays them in a Dropdown widget. The first\n",
        "    item in the dropdown serves as a default prompt to select a dataset.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Load a Parquet dataset and remove rows with null values\n",
        "    df = pl.read_parquet(\"./datasets.parquet\").drop_nulls()\n",
        "\n",
        "    # Create a list of items for the dropdown (name, link) from DataFrame rows\n",
        "    items = [(row[\"name\"], row[\"links\"]) for row in df.iter_rows(named=True)]\n",
        "\n",
        "    # Add a default item at the beginning of the dropdown list\n",
        "    items.insert(0, (\"Click to Select a Dataset\", \"\"))\n",
        "\n",
        "    # Create a Dropdown widget with the options and a description\n",
        "    dropdown = widgets.Dropdown(options=items, description='Datasets',\n",
        "                                layout={'width':'50%'})\n",
        "\n",
        "    # Attach the callback function to the dropdown's 'value' property\n",
        "    dropdown.observe(on_dropdown_change, names=['value'])\n",
        "\n",
        "    # Display the dropdown widget in the Jupyter Notebook\n",
        "    display(dropdown)\n"
      ],
      "metadata": {
        "id": "XPmiZ7dnLr2B"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to fetch datasets. You can change the number of pages (Default 30)."
      ],
      "metadata": {
        "id": "5WYKxXMLar0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_OF_PAGES = 30\n",
        "fetch_datasets(number_of_pages=NUMBER_OF_PAGES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQgJhxoSaqzs",
        "outputId": "198ea33d-5664-4a89-f70d-c63a930abcca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [06:03<00:00, 12.13s/it]\n",
            "\u001b[32m2023-10-16 16:09:06.834\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfetch_datasets\u001b[0m:\u001b[36m65\u001b[0m - \u001b[32m\u001b[1mDataFrame saved as ./datasets.parquet\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to Extract, Transform and Load a dataset"
      ],
      "metadata": {
        "id": "59lh7L0BtFyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "80c32960a24f4d50b746ff3156ff5530",
            "58bddeafd3b049baad94041858e3e577",
            "53af2bfa3ce141609da2f5843f88602b"
          ]
        },
        "id": "kTeYSba2oxZ0",
        "outputId": "bb31bfdd-86b5-46b4-d468-4c40100e2d07"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Datasets', layout=Layout(width='50%'), options=(('Click to Select a Dataset', ''), ('Ele…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80c32960a24f4d50b746ff3156ff5530"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\u001b[32m2023-10-16 16:09:27.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEXTRACT STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:27.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mSelected item URL: https://data.ferndalemi.gov/datasets/D3::grocery-stores.csv?where=1=1&outSR=%7B%22latestWkid%22%3A2898%2C%22wkid%22%3A2898%7D\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:27.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mSelected item name: Grocery Stores\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:27.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mSelected item URL (Formatted): https://data.ferndalemi.gov/datasets/D3::grocery-stores.csv\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:27.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mSelected item name (Formatted): grocery_stores\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-16 16:09:28.911\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m54\u001b[0m - \u001b[32m\u001b[1mData successfully fetched and saved at ./data/raw/grocery_stores_2023_10.csv\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:28.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtransform\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mTRANSFORM STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:28.924\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtransform\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1mData successfully transformed and saved at: ./data/interim/grocery_stores_2023_10.csv\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:28.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLOAD STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:40.794\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m36\u001b[0m - \u001b[32m\u001b[1mReport successfully created\u001b[0m\n",
            "\u001b[32m2023-10-16 16:09:40.799\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m41\u001b[0m - \u001b[32m\u001b[1mData successfully loaded at ./data/processed/grocery_stores_2023_10.csv\u001b[0m\n",
            "100%|██████████| 1/1 [00:13<00:00, 13.02s/it]\n",
            "\u001b[32m2023-10-16 16:09:40.807\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36metl_workflow\u001b[0m:\u001b[36m26\u001b[0m - \u001b[32m\u001b[1mETL COMPLETED!\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report has been saved to data/reports/grocery_stores_2023_10_report.html!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment and run the following cell to remove all files keeping some files"
      ],
      "metadata": {
        "id": "SVVxDSta3o36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all files in the interim directory except \"table.csv\"\n",
        "#!find ./data/interim/ -type f ! -name 'table.csv' -exec rm -f {} +\n",
        "\n",
        "# Remove all files in the raw directory except \"table.csv\"\n",
        "#!find ./data/raw/ -type f ! -name 'table.csv' -exec rm -f {} +\n",
        "\n",
        "# Remove all files in the processed directory except \"table.csv\"\n",
        "#!find ./data/processed/ -type f ! -name 'table.csv' -exec rm -f {} +\n",
        "\n",
        "# Remove all files in the reports directory except \"report.html\"\n",
        "#!find ./data/reports/ -type f ! -name 'report.html' -exec rm -f {} +"
      ],
      "metadata": {
        "id": "tBKDDQLOGIdz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment and run the following cell to remove all files"
      ],
      "metadata": {
        "id": "IJyCIOds3yZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf ./data/interim/*\n",
        "#!rm -rf ./data/raw/*\n",
        "#!rm -rf ./data/processed/*\n",
        "#!rm -rf ./data/reports/*"
      ],
      "metadata": {
        "id": "85XPAvqn9gVD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You can find more open data for data projects in the following links"
      ],
      "metadata": {
        "id": "LxjrakuSQXgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New York City open data\n",
        "\n",
        "https://opendata.cityofnewyork.us/\n",
        "\n",
        "dataMontgomery\n",
        "\n",
        "https://data.montgomerycountymd.gov/browse\n",
        "\n",
        "Consumer Financial Protection\n",
        "\n",
        "https://www.consumerfinance.gov/data-research/public-data-inventory/\n",
        "\n",
        "EDGE Geodata\n",
        "\n",
        "https://data-nces.opendata.arcgis.com/search?collection=Dataset\n",
        "\n",
        "Ferndale Open Data\n",
        "\n",
        "https://data.ferndalemi.gov/\n",
        "\n",
        "New York open data\n",
        "\n",
        "https://data.ny.gov/\n",
        "\n",
        "Los Angeles Open Data\n",
        "\n",
        "https://data.lacity.org/"
      ],
      "metadata": {
        "id": "egPFL6AoQZxN"
      }
    }
  ]
}