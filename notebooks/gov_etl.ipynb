{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGsgycOkw1WvXyYNLhdN9P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dcbcccdac34044aca9f82ec9e0585085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Click to Select a Dataset",
              "Electric Vehicle Population Data",
              "Lottery Powerball Winning Numbers: Beginning 2010",
              "Crime Data from 2020 to Present",
              "FDIC Failed Bank List",
              "U.S. Chronic Disease Indicators (CDI)",
              "Death rates for suicide, by sex, race, Hispanic origin, and age: United States",
              "Walkability Index",
              "Motor Vehicle Collisions - Crashes",
              "Supply Chain Greenhouse Gas Emission Factors v1.2 by NAICS-6",
              "Real Estate Sales 2001-2020 GL",
              "Alzheimer's Disease and Healthy Aging Data",
              "Air Quality",
              "Mental Health Care in the Last 4 Weeks",
              "Lottery Mega Millions Winning Numbers: Beginning 2002",
              "Crimes - 2001 to Present",
              "National Obesity By State",
              "Popular Baby Names",
              "Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System",
              "U.S. Electric Utility Companies and Rates: Look-up by Zipcode (2020)",
              "Drug overdose death rates, by drug type, sex, age, race, and Hispanic origin: United States",
              "Warehouse and Retail Sales",
              "Consumer Complaint Database",
              "Diabetes",
              "Crash Reporting - Drivers Data",
              "Public School Characteristics - Current",
              "Conditions Contributing to COVID-19 Deaths, by State and Age, Provisional 2020-2023",
              "Health conditions among children under age 18, by selected characteristics: United States",
              "Indicators of Anxiety or Depression Based on Reported Frequency of Symptoms During Last 7 Days",
              "Demographic Statistics By Zip Code",
              "Air Traffic Passenger Statistics",
              "Heart Disease Mortality Data Among US Adults (35+) by State/Territory and County",
              "Pittsburgh American Community Survey 2015 - Miscellaneous Data",
              "NYPD Shooting Incident Data (Historic)",
              "School Attendance by Student Group and District, 2021-2022",
              "NCHS - Leading Causes of Death: United States",
              "Meteorite Landings",
              "Obesity among children and adolescents aged 2–19 years, by selected characteristics: United States",
              "Border Crossing Entry Data",
              "Electric Vehicle Population Size History By County",
              "Traffic Crashes - Crashes",
              "Cancer Rates",
              "Air Quality Measures on the National Environmental Health Tracking Network",
              "Smart Location Database",
              "Allegheny County Employee Salaries",
              "Jobs NYC Postings",
              "Listing of Active Businesses",
              "Electric Vehicle Charging Stations",
              "Specific Chronic Conditions",
              "Traffic Violations",
              "Adoptable Pets",
              "Supply Chain Shipment Pricing Data",
              "NCHS - Death rates and life expectancy at birth",
              "Financial Well-Being in America (2017)",
              "Monarch Butterfly Habitat Restoration: Polygon (Feature Layer)",
              "Feed Grains Database",
              "Percent Change in Consumer Spending",
              "Accidental Drug Related Deaths 2012-2022",
              "COVID-19 Patient Data",
              "NYPD Arrest Data (Year to Date)",
              "Monthly Counts of Deaths by Select Causes, 2014-2019",
              "Drug Use Data from Selected Hospitals",
              "Adult Depression (LGHC Indicator)",
              "Provisional COVID-19 Deaths by HHS Region, Race, and Age",
              "Normal weight, overweight, and obesity among adults aged 20 and over, by selected characteristics: United States",
              "Grocery Stores",
              "Clean Air Status and Trends Network (CASTNET): Ozone",
              "Real Estate Across the United States (REXUS) Inventory (Building)",
              "Alternative Fueling Station Locations",
              "Rates and Trends in Heart Disease and Stroke Mortality Among US Adults (35+) by County, Age Group, Race/Ethnicity, and Sex – 2000-2019",
              "Employee Payroll",
              "Electric Vehicle Title and Registration Activity",
              "Zoning by Address",
              "SBS Certified Business List",
              "Retail Food Stores",
              "NYPD Arrests Data (Historic)",
              "Motor Carrier Registrations - Census Files",
              "National Health Interview Survey (NHIS) - National Cardiovascular Disease Surveillance Data",
              "Harmonized Tariff Schedule of the United States (2023)",
              "COVID-19 Daily Counts of Cases, Hospitalizations, and Deaths",
              "Power Outages - Zipcode",
              "Water Quality Data",
              "Behavioral Risk Factor Data: Tobacco Use (2011 to present)",
              "1.08 Crash Data Report (detail)",
              "Medicare Monthly Enrollment",
              "2010 Census Populations by Zip Code",
              "Average Daily Traffic Counts",
              "School Neighborhood Poverty Estimates - Current",
              "2012 SAT Results",
              "Youth Tobacco Survey (YTS) Data",
              "Fire Department Calls for Service",
              "2021 Yellow Taxi Trip Data",
              "Infectious Diseases by Disease, County, Year, and Sex",
              "Sex Offenders",
              "NYPD Hate Crimes",
              "Center for Medicare & Medicaid Services (CMS) , Medicare Claims data",
              "Nutrition, Physical Activity, and Obesity - American Community Survey",
              "Greenhouse Gas Emissions",
              "Recalls Data",
              "Licensed Drivers, by state, gender, and age group",
              "National Survey of Family Growth",
              "FHFA House Price Indexes (HPIs)",
              "Youth Behavior Risk Survey (High School)",
              "Sales Tax Collections by State",
              "Chemicals in Cosmetics",
              "Post-COVID Conditions",
              "Trips by Distance",
              "Total tonnage of commodites carried on commercial waterways by traffic type",
              "Demographics by Zip Code",
              "Cannabis Retail Sales by Week Ending",
              "Heart Disease Mortality Data Among US Adults (35+) by State/Territory and County – 2018-2020",
              "Electric Vehicle Population Size History",
              "Biodiversity by County - Distribution of Animals, Plants and Natural Communities",
              "Current Employment Statistics (CES)",
              "Employee Travel Data (Non-Local)",
              "Crime Data from 2010 to 2019",
              "Index, Violent, Property, and Firearm Rates By County: Beginning 1990",
              "Estimated Gasoline Sales: Beginning 1995",
              "MVA Vehicle Sales Counts by Month for Calendar Year 2002 through June 2023",
              "Solar Footprints in California",
              "Weekly Provisional Counts of Deaths by State and Select Causes, 2020-2023",
              "FS National Forests Dataset (US Forest Service Proclaimed Forests)",
              "Violence Reduction - Victims of Homicides and Non-Fatal Shootings",
              "Clean Air Status and Trends Network (CASTNET) Download Data Module",
              "Air Traffic Landings Statistics",
              "Tax Lien Sale Lists",
              "COVID-19 Cases, Tests, and Deaths by ZIP Code",
              "Baltimore City Employee Salaries",
              "COVID-19 Case Surveillance Public Use Data",
              "Uranium Location Database",
              "U.S. Electric Utility Companies and Rates: Look-up by Zipcode (2021)",
              "Allegheny County Tax Liens (Filings, Satisfactions, and Current Status)",
              "311 Data",
              "Street Names",
              "Pittsburgh American Community Survey 2014 - Miscellaneous Data",
              "Part 1 Crime Data",
              "PLACES: County Data (GIS Friendly Format), 2020 release",
              "Hospitalization Discharge Rates",
              "Summer Sports Experience",
              "Crash Data",
              "State of Oregon Social Media Sites",
              "Atlas of Rural and Small-Town America",
              "U.S. Life Expectancy at Birth by State and Census Tract - 2010-2015",
              "College Credit Card Marketing Agreements Data",
              "Sugar-Sweetened Beverage Consumption in California Residents",
              "Monthly Casino Slot Revenue for Current Year",
              "National Greenhouse Gas Emission Inventory",
              "Unintentional Drug Overdose Death Rate by Race/Ethnicity",
              "Global Landslide Catalog Export",
              "Behavioral Risk Factor Surveillance System (BRFSS) -  National Cardiovascular Disease Surveillance Data",
              "Good Food Purchasing Data",
              "State Energy Data System (SEDS)",
              "2013 - 2015 New York State Mathematics Exam",
              "Affordable Housing by Town 2011-2022",
              "COVID-19 Hospital Data",
              "Authorizations From 10/01/2006 Thru 3/31/2023",
              "Violence Reduction - Victim Demographics - Aggregated",
              "Financial Institutions",
              "COVID-19 Reported Patient Impact and Hospital Capacity by Facility -- RAW",
              "5.12 Cybersecurity (detail)",
              "911 Calls for Service 2021",
              "TSCA Inventory",
              "Spill Incidents",
              "Telemedicine Use in the Last 4 Weeks",
              "DIR Electrician Certification Unit (ECU)",
              "Iowa Liquor Sales",
              "NYC Taxi Zones",
              "Water Consumption And Cost (2013 - Feb 2023)",
              "Vehicle Fuel Type Count by Zip Code",
              "AH Provisional Diabetes Death Counts for 2020",
              "Citywide Payroll Data (Fiscal Year)",
              "National Vital Statistics System (NVSS) - National Cardiovascular Disease Surveillance Data",
              "Occupational Employment and Wage Statistics",
              "Nutrition, Physical Activity, and Obesity - Women, Infant, and Child",
              "Subway Stations",
              "Public School Locations 2021-22",
              "Vacation Rentals (Hotels, B&B, short-term rentals, etc.)",
              "Liquor Brands",
              "Report on U.S. Methane Emissions 1990-2020: Inventories, Projections, and Opportunities for Reductions: 2001 Updated emission and cost estimates",
              "Motor Vehicle Collisions - Vehicles",
              "Annual Crime Dataset 2015",
              "My Brother's Keeper Key Statistical Indicators on Boys and Men of Color",
              "Proportion of Adults Who Are Current Smokers (LGHC Indicator)",
              "Motor Vehicle Registrations Dashboard data",
              "Global Landslide Catalog",
              "Bedbug Reporting",
              "FAIN",
              "Police Department Incident Reports: 2018 to Present",
              "Lost, found, adoptable pets",
              "Public School Characteristics 2020-21",
              "Hospital Provider Cost Report"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select a dataset:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_7909195fe798449fae245efb2f9515e0",
            "style": "IPY_MODEL_4cc6f204bbe049c4981fb50d0f58552b"
          }
        },
        "7909195fe798449fae245efb2f9515e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cc6f204bbe049c4981fb50d0f58552b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathewsrc/Streamlined-ETL-Process-Unleashing-Polars-and-Dataprep/blob/master/notebooks/gov_etl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlined ETL Process: Unleashing Polars and Dataprep"
      ],
      "metadata": {
        "id": "xbeyh2FWetc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Summary:**\n",
        "\n",
        "This ETL (Extract, Transform, Load) project employs several Python libraries, including Polars, Dataprep, Requests, BeautifulSoup, and Loguru, to streamline the extraction, transformation, and loading of CSV datasets from the U.S. government's data repository at https://catalog.data.gov.\n",
        "\n"
      ],
      "metadata": {
        "id": "XQneUg0fClmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Objectives:**\n",
        "\n",
        "Extraction: I utilize the requests library and BeautifulSoup to scrape datasets from https://catalog.data.gov, a repository of various data formats, including CSV, XLS, and HTML.\n",
        "\n",
        "Transformation: Data manipulation and cleaning are accomplished using Polars, a high-performance data manipulation library written in Rust.\n",
        "\n",
        "Data Profiling: Dataprep is employed to create dynamic data reports and facilitate data profiling, quality assessment, and visualization, providing insights into data quality and characteristics.\n",
        "\n",
        "Loading: Transformed data is saved in CSV files using Polars.\n",
        "\n",
        "Logging: Loguru is chosen for logging, ensuring transparency and facilitating debugging throughout the ETL process.\n",
        "\n",
        "Through the automation of these ETL tasks, I establish a robust data pipeline that transforms raw data into valuable assets, supporting informed decision-making and data-driven insights.\n",
        "\n",
        "In this revised version, the content remains largely the same, but it is made more concise and clearer."
      ],
      "metadata": {
        "id": "qCxLL3DWAGLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directory structure**\n",
        "\n",
        "- **data/raw/**: Contains raw data files.\n",
        "- **data/interim/**: Contains intermediary processed data.\n",
        "- **data/processed/**: Contains processed data.\n",
        "- **data/reports/**: Contains reports for each dataset."
      ],
      "metadata": {
        "id": "EkWfyHr68Igk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "Ez3Aqqq3Vs7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q loguru==0.7.2\n",
        "!pip install -q polars==0.19.7\n",
        "!pip install -q furl==2.1.3\n",
        "!pip install -q tqdm==4.66.1\n",
        "!pip install -q dataprep==0.4.5\n",
        "!pip install -q requests==2.31.0\n",
        "!pip install -q pandas==2.1.1\n",
        "!pip install -q beautifulsoup4==4.12.2"
      ],
      "metadata": {
        "id": "dXYtgXthx9Jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f282110a-87cc-4286-b578-b7172a8371e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.2.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.6/133.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.0/764.0 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for metaphone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.3.24 which is incompatible.\n",
            "panel 1.2.3 requires bokeh<3.3.0,>=3.1.1, but you have bokeh 2.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataprep 0.4.5 requires pandas<2.0,>=1.1, but you have pandas 2.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.1 which is incompatible.\n",
            "panel 1.2.3 requires bokeh<3.3.0,>=3.1.1, but you have bokeh 2.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "5zJz-MTyVvwq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RkPBYbeGNFtn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import furl\n",
        "import polars as pl\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "from loguru import logger\n",
        "import sys\n",
        "import re\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import dataprep\n",
        "from dataprep.eda import create_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Filter out warnings throwed by dataprep\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "metadata": {
        "id": "9AZzkLhbY2km"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print package versions\n",
        "print(f\"requests version: {requests.__version__}\") #2.31.0\n",
        "print(f\"furl version: {furl.__version__}\") #2.1.3\n",
        "print(f\"polars version: {pl.__version__}\") #0.19.7\n",
        "print(f\"BeautifulSoup version: {bs4.__version__}\") #4.11.2\n",
        "print(f\"pandas version: {pd.__version__}\") #1.5.3\n",
        "print(f\"Python version: {sys.version}\")"
      ],
      "metadata": {
        "id": "0DqiqrW3xOT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90f4ceb-04d7-426f-ac7b-cb8f448c8c4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requests version: 2.31.0\n",
            "furl version: 2.1.3\n",
            "polars version: 0.19.7\n",
            "BeautifulSoup version: 4.12.2\n",
            "pandas version: 2.1.1\n",
            "Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create directories to save data"
      ],
      "metadata": {
        "id": "8-zSeK8sC464"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directories to save data\n",
        "\n",
        "raw_dir = \"./data/raw\"\n",
        "interim_dir = \"./data/interim\"\n",
        "processed_dir = \"./data/processed\"\n",
        "reports_dir = \"./data/reports\"\n",
        "\n",
        "# Create directories\n",
        "for dir in [raw_dir, interim_dir, processed_dir, reports_dir]:\n",
        "    os.makedirs(dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "lj5YxgIm6vWv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define files time period to improve organization"
      ],
      "metadata": {
        "id": "-pAojves7f5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the current date and time\n",
        "current_datetime = datetime.now()\n",
        "\n",
        "# Format the current date and time to obtain the time period as \"YYYY-MM\"\n",
        "time_period = current_datetime.strftime(\"%Y_%m\")\n",
        "logger.info(f\"Time period: {time_period}\")"
      ],
      "metadata": {
        "id": "Z92dUv__nFqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828f0e88-607a-4353-b431-04846956f51e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-14 17:42:33.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 6>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mTime period: 2023_10\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fetch datasets from\n",
        "\n",
        "https://catalog.data.gov/dataset/"
      ],
      "metadata": {
        "id": "rxipnksUS-sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "\n",
        "@logger.catch\n",
        "def fetch_datasets(number_of_pages=3):\n",
        "    \"\"\"\n",
        "    Fetch datasets from data.gov catalog web pages and save them to a Parquet file.\n",
        "\n",
        "    Args:\n",
        "        number_of_pages (int, optional): The number of catalog pages to scrape. Default is 3.\n",
        "\n",
        "    This function scrapes dataset information from data.gov catalog web pages,\n",
        "    extracts dataset names, organizations, descriptions, and CSV links to dataset\n",
        "    resources, and then saves the data as a Parquet file.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "\n",
        "    for page in tqdm(range(1, number_of_pages + 1)):\n",
        "        url = f\"https://catalog.data.gov/dataset/?page={page}\"\n",
        "\n",
        "        # Send an HTTP GET request to the specified URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the response status code is 200 (OK)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the web page using BeautifulSoup\n",
        "            bs = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            ul_tag = bs.find(\"ul\", class_=\"dataset-list unstyled\")\n",
        "\n",
        "            if ul_tag:\n",
        "                li_tags = bs.find_all(\"li\", class_=\"dataset-item has-organization\")\n",
        "                for li in li_tags:\n",
        "                    # Extract the dataset name\n",
        "                    name = li.find('h3').find('a').text\n",
        "\n",
        "                    # Extract the dataset organization\n",
        "                    organization = li.find('p', class_='dataset-organization').text.replace(\"—\", \"\")\n",
        "\n",
        "                    # Extract the dataset description\n",
        "                    description = li.find('div', class_='notes').find('div').text\n",
        "\n",
        "                    # Extract CSV links to dataset resources\n",
        "                    links = [a['href'] for a in li.find_all('a', class_='label label-default', attrs={\"data-format\": \"csv\"})]\n",
        "\n",
        "                    # Add dataset if it contains links\n",
        "                    if len(links) > 0:\n",
        "                        datasets.append({\n",
        "                            \"name\": name,\n",
        "                            \"organization\": organization,\n",
        "                            \"description\": description,\n",
        "                            \"links\": links\n",
        "                        })\n",
        "        else:\n",
        "            logger.error(\"Failed to fetch the web page.\")\n",
        "\n",
        "    # Save the collected dataset information as a Parquet file\n",
        "    pl.LazyFrame(datasets).sink_parquet(\"datasets.parquet\")\n",
        "\n",
        "    logger.success(\"DataFrame saved as ./datasets.parquet\")\n"
      ],
      "metadata": {
        "id": "4UqrMcg-S-F1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_OF_PAGES = 20 # number of pages to fetch data\n",
        "fetch_datasets(number_of_pages=NUMBER_OF_PAGES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_g9xQ0EmfyW",
        "outputId": "224bc916-5307-4592-f969-aca65e3a3713"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:35<00:00,  7.78s/it]\n",
            "\u001b[32m2023-10-14 17:45:08.712\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfetch_datasets\u001b[0m:\u001b[36m65\u001b[0m - \u001b[32m\u001b[1mDataFrame saved as ./datasets.parquet\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract"
      ],
      "metadata": {
        "id": "_vOPFIqJDb7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "@logger.catch\n",
        "def format_url(url):\n",
        "    \"\"\"\n",
        "    Format URL\n",
        "\n",
        "    Args:\n",
        "        url (str): The original URL to be formatted.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted URL.\n",
        "\n",
        "    Example:\n",
        "        >>> format_url(\"https://www.ers.usda.gov/page?q=example\")\n",
        "        'https://www.ers.usda.gov/page'\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Define a regular expression pattern to remove query parameters from the URL\n",
        "    pattern = r'\\?.+'\n",
        "\n",
        "    # Use regular expression substitution to remove the pattern from the URL\n",
        "    url = re.sub(pattern, '', url)\n",
        "\n",
        "     # Define a prefix for the URL\n",
        "    prefix = \"https://www.ers.usda.gov\"\n",
        "\n",
        "    # Add prefix if URL does not start with https:// or http://\n",
        "    if not url.startswith(\"https://\") and not url.startswith(\"http://\"):\n",
        "        url = furl.furl(prefix).add(path=url).url\n",
        "\n",
        "    return url"
      ],
      "metadata": {
        "id": "bb9k_JSp1-pq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assert that `format_url()` is working as expected\n"
      ],
      "metadata": {
        "id": "Mux2fn252knk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function with assertions\n",
        "assert format_url(\"https://www.ers.usda.gov/page?q=example\") == 'https://www.ers.usda.gov/page'\n",
        "assert format_url(\"https://www.ers.usda.gov/\") == 'https://www.ers.usda.gov/'\n",
        "assert format_url(\"/webdocs/DataFiles/106595/Dates2020.csv?v=5007.3\") == \"https://www.ers.usda.gov/webdocs/DataFiles/106595/Dates2020.csv\"\n",
        "assert format_url(\"/webdocs/DataFiles/51035/FruitPrices2020.csv?v=5007\") == \"https://www.ers.usda.gov/webdocs/DataFiles/51035/FruitPrices2020.csv\"\n",
        "assert format_url(\"https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD\") == \"https://data.wa.gov/api/views/f6w7-q2d2/rows.csv\""
      ],
      "metadata": {
        "id": "ZZanaEq5D-4f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "@logger.catch\n",
        "def modify_file_name(name):\n",
        "    \"\"\"\n",
        "    Modify a file name by performing the following transformations:\n",
        "\n",
        "    Args:\n",
        "        name (str): The original file name to be modified.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified file name.\n",
        "\n",
        "    Example:\n",
        "        >>> modify_file_name(\"My File 123.txt\")\n",
        "        'my_file_123.txt'\n",
        "\n",
        "        >>> modify_file_name(\"!@#File Name$%^\")\n",
        "        'file_name'\n",
        "\n",
        "        >>> modify_file_name(\"File123\")\n",
        "        'file123'\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the file name to lowercase\n",
        "    name = name.lower()\n",
        "\n",
        "    # Replace non-alphanumeric and non-underscore characters with underscores\n",
        "    name = re.sub(r'[^a-zA-Z0-9_\\.]+', '_', name)\n",
        "\n",
        "    # Remove non-alphanumeric characters from the start and end of the name\n",
        "    name = re.sub(r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9]+$', '', name)\n",
        "\n",
        "    return name\n"
      ],
      "metadata": {
        "id": "6pRBf0s9CVe_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert modify_file_name(\"My File 123.txt\") == \"my_file_123.txt\"\n",
        "assert modify_file_name(\"!@#File Name$%^\") == \"file_name\"\n",
        "assert modify_file_name(\"File123\") == \"file123\"\n",
        "assert modify_file_name(\"   leading_trailing   \") == \"leading_trailing\"\n",
        "assert modify_file_name(\"_#Name_With_Underscores_%\") == \"name_with_underscores\""
      ],
      "metadata": {
        "id": "2f4h5_j5Chs1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@logger.catch\n",
        "def extract(url, name):\n",
        "    \"\"\"\n",
        "    Extract data from a specified URL, format the URL and file name,\n",
        "    and save the data to a file.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the data source.\n",
        "        name (str): The name of the data item.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the saved data file, or None if extraction fails.\n",
        "\n",
        "    Example:\n",
        "        >>> extract(\"https://www.example.com/data.csv\", \"Sample Data\")\n",
        "        'path_to_saved_data/Sample_Data.csv'\n",
        "\n",
        "        >>> extract(\"https://www.example.com/data.txt\", \"Text Data\")\n",
        "        # Error: Unsupported file format\n",
        "    \"\"\"\n",
        "\n",
        "    logger.info(\"\\nEXTRACT STARTED!\")\n",
        "\n",
        "    # Log the URL and name of the selected item\n",
        "    logger.info(f'Selected item URL: {url}')\n",
        "    logger.info(f'Selected item name: {name}')\n",
        "\n",
        "    # Format the file name\n",
        "    name = modify_file_name(name)\n",
        "\n",
        "    # Format the URL\n",
        "    url = format_url(url)\n",
        "\n",
        "    # Log the formatted URL and name\n",
        "    logger.info(f'Selected item URL (Formatted): {url}')\n",
        "    logger.info(f'Selected item name (Formatted): {name}')\n",
        "\n",
        "    # Determine the file extension from the URL\n",
        "    file_extension = url.split('.')[-1].lower()\n",
        "\n",
        "    # Generate the output file name\n",
        "    filename = f\"{name}_{time_period}.{file_extension}\"\n",
        "\n",
        "    # Handle CSV files\n",
        "    if file_extension == 'csv':\n",
        "        output_path = os.path.join(raw_dir, filename)\n",
        "        pl.read_csv(url, try_parse_dates=True, ignore_errors=True).write_csv(output_path)\n",
        "    else:\n",
        "        logger.error(f\"Unsupported file format: {file_extension}\")\n",
        "        return None\n",
        "\n",
        "    # Log success and return the path to the saved file\n",
        "    logger.success(f\"Data successfully fetched and saved at {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "pO54eEbDNwGi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if `extract()` function is working as expected"
      ],
      "metadata": {
        "id": "nT3FY1vPDfRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD\"\n",
        "expected_output_file = f\"electric_vehicle_population_data_{time_period}.csv\"\n",
        "expected_output_path = os.path.join(raw_dir, expected_output_file)\n",
        "extracted_output_path =  extract(url, \"Electric Vehicle Population Data\")\n",
        "assert extracted_output_path == expected_output_path"
      ],
      "metadata": {
        "id": "EWQo6T1Z9BSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735a1101-77c1-4eb8-b900-552ec8b07af3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-14 17:45:08.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1m\n",
            "EXTRACT STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-14 17:45:08.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mSelected item URL: https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD\u001b[0m\n",
            "\u001b[32m2023-10-14 17:45:08.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mSelected item name: Electric Vehicle Population Data\u001b[0m\n",
            "\u001b[32m2023-10-14 17:45:08.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mSelected item URL (Formatted): https://data.wa.gov/api/views/f6w7-q2d2/rows.csv\u001b[0m\n",
            "\u001b[32m2023-10-14 17:45:08.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mSelected item name (Formatted): electric_vehicle_population_data\u001b[0m\n",
            "\u001b[32m2023-10-14 17:45:14.935\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m53\u001b[0m - \u001b[32m\u001b[1mData successfully fetched and saved at ./data/raw/electric_vehicle_population_data_2023_10.csv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform"
      ],
      "metadata": {
        "id": "R8n2vQqKDnwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "@logger.catch\n",
        "def lower_column(df):\n",
        "    \"\"\"\n",
        "    Rename columns to lowercase in a Polars DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pl.DataFrame): The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: The Polars DataFrame with column names in lowercase.\n",
        "\n",
        "    Example:\n",
        "        >>> df = pl.DataFrame({\n",
        "        ...     \"Column1\": [1, 2, 3],\n",
        "        ...     \"Column2\": [\"A\", \"B\", \"C\"]\n",
        "        ... })\n",
        "        >>> df = lower_column(df)\n",
        "        >>> print(df)\n",
        "        shape: (3, 2)\n",
        "        ┌────────┬────────┐\n",
        "        │ column1│ column2│\n",
        "        │ int    │ str    │\n",
        "        ╞════════╪════════╡\n",
        "        │ 1      │ \"A\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 2      │ \"B\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 3      │ \"C\"    │\n",
        "        └────────┴────────┘\n",
        "\n",
        "    Note:\n",
        "        This function renames columns in the input Polars DataFrame to lowercase.\n",
        "        It creates a new DataFrame with column names converted to lowercase while keeping the original data intact.\n",
        "\n",
        "    \"\"\"\n",
        "    return df.select([pl.col(col).alias(col.lower().replace(\" \", \"_\")) for col in df.columns])"
      ],
      "metadata": {
        "id": "mhMT9GcvqbWN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "@logger.catch\n",
        "def drop_duplicates(df):\n",
        "    \"\"\"\n",
        "    Remove duplicate rows from a Polars DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pl.DataFrame): The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: The Polars DataFrame with duplicate rows removed.\n",
        "\n",
        "    Example:\n",
        "        >>> df = pl.DataFrame({\n",
        "        ...     \"Column1\": [1, 2, 2, 3, 4],\n",
        "        ...     \"Column2\": [\"A\", \"B\", \"B\", \"C\", \"D\"]\n",
        "        ... })\n",
        "        >>> df = drop_duplicates(df)\n",
        "        >>> print(df)\n",
        "        shape: (4, 2)\n",
        "        ┌────────┬────────┐\n",
        "        │ column1│ column2│\n",
        "        │ int    │ str    │\n",
        "        ╞════════╪════════╡\n",
        "        │ 1      │ \"A\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 2      │ \"B\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 3      │ \"C\"    │\n",
        "        ├────────┼────────┤\n",
        "        │ 4      │ \"D\"    │\n",
        "        └────────┴────────┘\n",
        "\n",
        "    Note:\n",
        "        This function removes duplicate rows from the input Polars DataFrame.\n",
        "        It creates a new DataFrame with duplicate rows removed, but it does not modify the original DataFrame.\n",
        "\n",
        "    \"\"\"\n",
        "    return df.unique(keep=\"first\")"
      ],
      "metadata": {
        "id": "U15jenvIqe4n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "def drop_full_row_null(df):\n",
        "    \"\"\"\n",
        "    Drop rows from a polars DataFrame if all values in a row are null.\n",
        "\n",
        "    This function takes a polars DataFrame and removes rows where all values in the row are null.\n",
        "\n",
        "    Args:\n",
        "        df (pl.DataFrame): The input polars DataFrame from which rows with all-null values will be removed.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: A new polars DataFrame with rows removed if all values in the row are null.\n",
        "\n",
        "    Example:\n",
        "        >>> import polars as pl\n",
        "        >>> df = pl.DataFrame({\n",
        "        ...     'A': [1, 2, None, 4],\n",
        "        ...     'B': [None, None, None, None]\n",
        "        ... })\n",
        "        >>> result = drop_full_row_null(df)\n",
        "        >>> print(result)\n",
        "        shape: (4, 2)\n",
        "        ┌─────┬─────┐\n",
        "        │ A   │ B   │\n",
        "        │ --- │ --- │\n",
        "        │ i64 │ i64 │\n",
        "        ╞═════╪═════╡\n",
        "        │ 1   │ null│\n",
        "        ├─────┼─────┤\n",
        "        │ 2   │ null│\n",
        "        ├─────┼─────┤\n",
        "        │ null│ null│\n",
        "        ├─────┼─────┤\n",
        "        │ 4   │ null│\n",
        "        └─────┴─────┘\n",
        "    \"\"\"\n",
        "    return df.filter(~pl.all_horizontal(pl.all().is_null()))\n"
      ],
      "metadata": {
        "id": "N--St4QLqiq-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "@logger.catch\n",
        "def transform(input_path, name):\n",
        "    \"\"\"\n",
        "    Transform a CSV file by converting all text columns to lowercase and save the result.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The path to the input CSV file to be transformed.\n",
        "        name (str): A descriptive name for the transformation process.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the transformed CSV file.\n",
        "\n",
        "    Example:\n",
        "        >>> transform(\"input_data.csv\", \"Lowercase Conversion\")\n",
        "        'path_to_transformed_data/input_data.csv'\n",
        "    \"\"\"\n",
        "    logger.info(\"TRANSFORM STARTED!\")\n",
        "\n",
        "    # Determine the file extension from the filename\n",
        "    file_extension = input_path.split('.')[-1].lower()\n",
        "\n",
        "\n",
        "    # Extract the file name from the path\n",
        "    file_name = os.path.basename(input_path)\n",
        "\n",
        "    # Define the output file path\n",
        "    output_path = os.path.join(interim_dir, file_name)\n",
        "\n",
        "    # Read the CSV file as LazyFrame using polars\n",
        "    df = pl.scan_csv(input_path)\n",
        "\n",
        "    df = (df.pipe(lower_column)  # Convert all text columns to lowercase\n",
        "            .pipe(drop_duplicates)   # Drop duplicate rows\n",
        "            .pipe(drop_full_row_null)  # Drop a row only if all values are null\n",
        "    )\n",
        "\n",
        "    # Save the transformed data as a CSV file\n",
        "    df.collect().write_csv(output_path)\n",
        "\n",
        "    # Log success and return the path to the transformed file\n",
        "    logger.success(f\"Data successfully transformed and saved at: {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "1EEhG8TVOobk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if `transform()` function is working as expected"
      ],
      "metadata": {
        "id": "NFclR4vADxKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expect_transformed_output_path = './data/interim/electric_vehicle_population_data_2023_10.csv'\n",
        "name = f\"electric_vehicle_population_data_{time_period}.csv\"\n",
        "input_path = os.path.join(raw_dir, expected_output_file)\n",
        "transformed_output_path = transform(input_path, name)\n",
        "assert os.path.exists(expect_transformed_output_path)\n",
        "assert transformed_output_path == expect_transformed_output_path"
      ],
      "metadata": {
        "id": "84U1U2lTPLyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc7861a-bce1-41fa-b2c9-dbf48ec3e0b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-14 17:45:14.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtransform\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mTRANSFORM STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-14 17:45:15.288\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtransform\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1mData successfully transformed and saved at: ./data/interim/electric_vehicle_population_data_2023_10.csv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load"
      ],
      "metadata": {
        "id": "jHvYXeE3D38g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@logger.catch\n",
        "def load(input_path, name, report=True):\n",
        "    \"\"\"\n",
        "    Load data from a CSV file and save it to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The path to the input CSV file to be loaded.\n",
        "        name (str): A descriptive name for the loading process.\n",
        "        report (bool, optional): If True, a report will be created and saved. Default is True.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the loaded CSV file.\n",
        "\n",
        "    Example:\n",
        "        >>> load(\"input_data.csv\", \"Data Loading\")\n",
        "        'path_to_processed_data/input_data.csv'\n",
        "    \"\"\"\n",
        "    logger.info(\"LOAD STARTED!\")\n",
        "\n",
        "    # Determine the file format based on the filename extension\n",
        "    file_extension = input_path.split('.')[-1].lower()\n",
        "\n",
        "    # Extract the file name from the path\n",
        "    file_name = os.path.basename(input_path)\n",
        "\n",
        "    # Define the output file path in the processed directory\n",
        "    output_path = os.path.join(processed_dir, file_name)\n",
        "\n",
        "    # Read the CSV file using polars and save it to the processed directory\n",
        "    df = pl.scan_csv(input_path).collect()\n",
        "\n",
        "    if report:\n",
        "        # Create and save report\n",
        "        create_report(df.to_pandas(), progress=False).save(os.path.join(\n",
        "            reports_dir, f\"{file_name.split('.')[0]}_report.html\"))\n",
        "        logger.success(\"Report successfully created\")\n",
        "\n",
        "    df.write_csv(output_path)\n",
        "\n",
        "    # Log success and return the path to the loaded file\n",
        "    logger.success(f\"Data successfully loaded at {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "hBeBX9QlOV9-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if `load()` function is working as expected"
      ],
      "metadata": {
        "id": "0fP0YgtXD57Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expect_loaded_output_path = './data/processed/electric_vehicle_population_data_2023_10.csv'\n",
        "input_path = './data/interim/electric_vehicle_population_data_2023_10.csv'\n",
        "name = f\"electric_vehicle_population_data_{time_period}.csv\"\n",
        "loaded_output_path = load(input_path, name, False)\n",
        "assert  os.path.exists(expect_loaded_output_path)\n",
        "assert  loaded_output_path == expect_loaded_output_path"
      ],
      "metadata": {
        "id": "ptmtJswVSeyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9187d1-a377-45a7-ece7-a14711ff2bd5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-10-14 17:45:15.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLOAD STARTED!\u001b[0m\n",
            "\u001b[32m2023-10-14 17:45:15.531\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m41\u001b[0m - \u001b[32m\u001b[1mData successfully loaded at ./data/processed/electric_vehicle_population_data_2023_10.csv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all together"
      ],
      "metadata": {
        "id": "WzQj_WCuVmcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "@logger.catch\n",
        "def etl_workflow(tagged_urls):\n",
        "    \"\"\"\n",
        "    Perform Extract, Transform, and Load (ETL) process for a list of tagged URLs.\n",
        "\n",
        "    Args:\n",
        "        tagged_urls (list): A list of tuples, each containing a dataset name and its URL.\n",
        "\n",
        "    The function iterates through the tagged URLs, extracts data, applies transformation,\n",
        "    and loads the data. The ETL process is performed for each dataset in the list.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    for name, url in tqdm(tagged_urls):\n",
        "        output_path = extract(url, name)\n",
        "        if output_path is None:\n",
        "            continue\n",
        "        output_path = transform(output_path, name)\n",
        "        if output_path is None:\n",
        "            continue\n",
        "        load(output_path, name)\n",
        "\n",
        "    logger.success(\"ETL COMPLETED!\")"
      ],
      "metadata": {
        "id": "i56QmgZ7ycoz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from loguru import logger\n",
        "\n",
        "@logger.catch\n",
        "def on_dropdown_change(change):\n",
        "    \"\"\"\n",
        "    Callback function triggered when the dropdown value changes.\n",
        "\n",
        "    Args:\n",
        "        change (ipywidgets.widgets.widget.CallbackDispatcher.ChangeTrait): The change event.\n",
        "\n",
        "    This function is called when the value of the dropdown widget changes. It checks if\n",
        "    the new value is empty and, if not, initiates the ETL (Extract, Transform, Load) workflow\n",
        "    for the selected dataset.\n",
        "\n",
        "    Args:\n",
        "        change (ipywidgets.widgets.widget.CallbackDispatcher.ChangeTrait): The change event object.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    selected_option = change.owner.options[change.owner.index]\n",
        "\n",
        "    # Check if the new value is empty\n",
        "    if change.new == \"\":\n",
        "        return\n",
        "\n",
        "    name, urls = selected_option[0], change.new\n",
        "\n",
        "    for url in urls:\n",
        "        etl_workflow([(name, url)])"
      ],
      "metadata": {
        "id": "KUBq4JZGma0K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "@logger.catch\n",
        "def show_datasets():\n",
        "    \"\"\"\n",
        "    Display a dropdown widget containing a list of datasets loaded from a Parquet file.\n",
        "\n",
        "    This function loads a Parquet dataset, removes rows with null values, creates a list of items\n",
        "    (dataset name, links) from DataFrame rows, and displays them in a Dropdown widget. The first\n",
        "    item in the dropdown serves as a default prompt to select a dataset.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Load a Parquet dataset and remove rows with null values\n",
        "    df = pl.read_parquet(\"./datasets.parquet\").drop_nulls()\n",
        "\n",
        "    # Create a list of items for the dropdown (name, link) from DataFrame rows\n",
        "    items = [(row[\"name\"], row[\"links\"]) for row in df.iter_rows(named=True)]\n",
        "\n",
        "    # Add a default item at the beginning of the dropdown list\n",
        "    items.insert(0, (\"Click to Select a Dataset\", \"\"))\n",
        "\n",
        "    # Create a Dropdown widget with the options and a description\n",
        "    dropdown = widgets.Dropdown(options=items, description='Select a dataset:')\n",
        "\n",
        "    # Attach the callback function to the dropdown's 'value' property\n",
        "    dropdown.observe(on_dropdown_change, names=['value'])\n",
        "\n",
        "    # Display the dropdown widget in the Jupyter Notebook\n",
        "    display(dropdown)\n"
      ],
      "metadata": {
        "id": "XPmiZ7dnLr2B"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to Extract, Transform and Load a dataset"
      ],
      "metadata": {
        "id": "59lh7L0BtFyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dcbcccdac34044aca9f82ec9e0585085",
            "7909195fe798449fae245efb2f9515e0",
            "4cc6f204bbe049c4981fb50d0f58552b"
          ]
        },
        "id": "kTeYSba2oxZ0",
        "outputId": "f3c63769-8eb0-4e2f-ecb1-52f6c311b24b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select a dataset:', options=(('Click to Select a Dataset', ''), ('Electric Vehicle Popul…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcbcccdac34044aca9f82ec9e0585085"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to remove all files keeping some files"
      ],
      "metadata": {
        "id": "SVVxDSta3o36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all files in the interim directory except \"table.csv\"\n",
        "#!find ./data/interim/ -type f ! -name 'table.csv' -exec rm -f {} +\n",
        "\n",
        "# Remove all files in the raw directory except \"table.csv\"\n",
        "#!find ./data/raw/ -type f ! -name 'table.csv' -exec rm -f {} +\n",
        "\n",
        "# Remove all files in the processed directory except \"table.csv\"\n",
        "#!find ./data/processed/ -type f ! -name 'table.csv' -exec rm -f {} +\n",
        "\n",
        "# Remove all files in the reports directory except \"report.html\"\n",
        "#!find ./data/reports/ -type f ! -name 'report.html' -exec rm -f {} +"
      ],
      "metadata": {
        "id": "tBKDDQLOGIdz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to remove all files"
      ],
      "metadata": {
        "id": "IJyCIOds3yZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./data/interim/*\n",
        "!rm -rf ./data/raw/*\n",
        "!rm -rf ./data/processed/*\n",
        "!rm -rf ./data/reports/*"
      ],
      "metadata": {
        "id": "85XPAvqn9gVD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You can find more open data for data projects in the following links"
      ],
      "metadata": {
        "id": "LxjrakuSQXgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New York City open data\n",
        "\n",
        "https://opendata.cityofnewyork.us/\n",
        "\n",
        "dataMontgomery\n",
        "\n",
        "https://data.montgomerycountymd.gov/browse\n",
        "\n",
        "Consumer Financial Protection\n",
        "\n",
        "https://www.consumerfinance.gov/data-research/public-data-inventory/\n",
        "\n",
        "EDGE Geodata\n",
        "\n",
        "https://data-nces.opendata.arcgis.com/search?collection=Dataset\n",
        "\n",
        "Ferndale Open Data\n",
        "\n",
        "https://data.ferndalemi.gov/\n",
        "\n",
        "New York open data\n",
        "\n",
        "https://data.ny.gov/\n",
        "\n",
        "Los Angeles Open Data\n",
        "\n",
        "https://data.lacity.org/"
      ],
      "metadata": {
        "id": "egPFL6AoQZxN"
      }
    }
  ]
}